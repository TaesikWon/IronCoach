# backend/app/services/ai_chat_service.py
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.schema.runnable import RunnableLambda  # âœ… ì¶”ê°€
from app.services.ai_service import (
    compressed_retriever,   # RAG ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ê¸°
    refine_text,            # GPT-4o-mini ë¬¸ì²´ ë‹¤ë“¬ê¸°
    generate_response,      # GPT2 ë¡œì»¬ ì‘ë‹µ í•¨ìˆ˜
)

# ---------------------------
# ğŸ§  ëŒ€í™” ë©”ëª¨ë¦¬ (ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ì–µ)
# ---------------------------
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# ---------------------------
# ğŸ’¬ ë¡œì»¬ LLM (GPT2 ê¸°ë°˜)
# ---------------------------
local_llm = RunnableLambda(lambda prompt: generate_response(prompt))

# ---------------------------
# ğŸ’¬ ëŒ€í™”í˜• ì²´ì¸ êµ¬ì„±
# ---------------------------
chat_chain = ConversationalRetrievalChain.from_llm(
    llm=local_llm,                  # âœ… LocalLLMWrapper ëŒ€ì‹  RunnableLambda ì‚¬ìš©
    retriever=compressed_retriever, # âœ… RAG + ë¦¬ë­í‚¹
    memory=memory,                  # âœ… ëŒ€í™” ê¸°ë¡ ì €ì¥
    verbose=False
)

# ---------------------------
# ğŸ—£ï¸ ë©”ì¸ í•¨ìˆ˜
# ---------------------------
async def chat_with_coach(user_message: str) -> str:
    """
    ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ ë°›ì•„ RAG + LLMì„ í†µí•´
    AI ì½”ì¹˜ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # 1ï¸âƒ£ RAG + GPT2 ê¸°ë°˜ 1ì°¨ ì‘ë‹µ ìƒì„±
    raw_output = chat_chain.run(user_message)

    # 2ï¸âƒ£ GPT-4o-minië¡œ ë¬¸ì²´ ë‹¤ë“¬ê¸° (ë¹„ë™ê¸°)
    refined_output = await refine_text(raw_output)

    # 3ï¸âƒ£ ìµœì¢… ì‘ë‹µ ë°˜í™˜
    return refined_output
