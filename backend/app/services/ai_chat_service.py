# backend/app/services/ai_chat_service.py
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.schema.runnable import RunnableLambda
from app.services.ai_service import (
    compressed_retriever,   # RAG ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ê¸°
    refine_text,            # GPT-4o-mini ë¬¸ì²´ ë‹¤ë“¬ê¸°
    generate_response,      # GPT2 ë¡œì»¬ ì‘ë‹µ í•¨ìˆ˜
)

# ---------------------------
# ğŸ§  ëŒ€í™” ë©”ëª¨ë¦¬ (ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ì–µ)
# ---------------------------
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# ---------------------------
# ğŸ’¬ ë¡œì»¬ LLM (GPT2 ê¸°ë°˜)
# ---------------------------
# âœ… LangChainì´ ìë™ìœ¼ë¡œ ë„˜ê¸°ëŠ” stop, metadata ë“±ì„ ë¬´ì‹œí•˜ë„ë¡ ìˆ˜ì •
local_llm = RunnableLambda(lambda prompt, **kwargs: generate_response(prompt))

# ---------------------------
# ğŸ’¬ ëŒ€í™”í˜• ì²´ì¸ êµ¬ì„±
# ---------------------------
chat_chain = ConversationalRetrievalChain.from_llm(
    llm=local_llm,
    retriever=compressed_retriever,
    memory=memory,
    verbose=False
)

# ---------------------------
# ğŸ—£ï¸ ë©”ì¸ í•¨ìˆ˜
# ---------------------------
async def chat_with_coach(user_message: str) -> str:
    """
    ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ ë°›ì•„ RAG + LLMì„ í†µí•´
    AI ì½”ì¹˜ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        # âœ… ìµœì‹  ë²„ì „ì—ì„œëŠ” run() ëŒ€ì‹  invoke() ì‚¬ìš©
        result = chat_chain.invoke({"question": user_message})
        raw_output = result["answer"] if isinstance(result, dict) else result

        # âœ… GPT-4o-minië¡œ ë¬¸ì²´ ë‹¤ë“¬ê¸°
        refined_output = await refine_text(raw_output)

        return refined_output

    except Exception as e:
        print(f"[âŒ Chat Error] {e}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì½”ì¹˜ ì‹œìŠ¤í…œì— ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
