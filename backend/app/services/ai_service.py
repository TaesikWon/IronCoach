# backend/app/services/ai_service.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from sentence_transformers import CrossEncoder
from langchain.schema.runnable import RunnableLambda
from openai import AsyncOpenAI
from app.core.config import settings
from app.core.database import SessionLocal
from app.models.session import Feedback

# ---------------------------
# ğŸ§  Polyglot-Ko-1.3B ë¡œì»¬ ëª¨ë¸ ë¡œë“œ
# ---------------------------
print("ğŸ”¹ Polyglot-Ko-1.3B ëª¨ë¸ ë¡œë“œ ì¤‘...")
model_id = "EleutherAI/polyglot-ko-1.3b"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype="auto",
    device_map="auto"
)

def generate_response(prompt: str) -> str:
    """Polyglot-Ko-1.3B ê¸°ë°˜ ë¡œì»¬ LLM ì‘ë‹µ ìƒì„±"""
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# ---------------------------
# ğŸ§© ChromaDB + ë¦¬ë­í‚¹ ì„¤ì •
# ---------------------------
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
db = Chroma(persist_directory="vectorstore", embedding_function=embeddings)
retriever = db.as_retriever(search_kwargs={"k": 10})

cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
reranker = CrossEncoderReranker.construct(model=cross_encoder, top_n=3)

compressed_retriever = ContextualCompressionRetriever(
    base_retriever=retriever,
    base_compressor=reranker
)

# ---------------------------
# ğŸ’¬ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
# ---------------------------
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""ë‹¹ì‹ ì€ ì „ë¬¸ íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤.
ì•„ë˜ì˜ ì„¸ì…˜ ê¸°ë¡(context)ì„ ì°¸ê³ í•˜ì—¬ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.

[ì„¸ì…˜ ê¸°ë¡]
{context}

[ì§ˆë¬¸]
{question}

1. í›ˆë ¨ ê°•ë„ í‰ê°€
2. ìì„¸ ë° íš¨ìœ¨ ê°œì„  íŒ
3. ë‹¤ìŒ í›ˆë ¨ ì œì•ˆ
""",
)

# ---------------------------
# ğŸ§  LLM ë˜í¼
# ---------------------------
def local_llm(prompt: str) -> str:
    return generate_response(prompt)

LocalLLM = RunnableLambda(local_llm)

# ---------------------------
# ğŸ’¬ RetrievalQA ì²´ì¸ êµ¬ì„±
# ---------------------------
qa_chain = RetrievalQA.from_chain_type(
    llm=LocalLLM,
    chain_type="stuff",
    retriever=compressed_retriever,
    chain_type_kwargs={"prompt": prompt},
)

# ---------------------------
# ğŸª„ GPT-4o-mini ë¬¸ì²´ ë‹¤ë“¬ê¸°
# ---------------------------
client = AsyncOpenAI(api_key=settings.openai_api_key)

async def refine_text(text: str) -> str:
    """OpenAI GPT ëª¨ë¸ë¡œ ë¬¸ì²´ ìì—°ìŠ¤ëŸ½ê²Œ ë³´ì •"""
    response = await client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ìš´ë™ ì½”ì¹­ í”¼ë“œë°±ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ì–´ ì£¼ì„¸ìš”."},
            {"role": "user", "content": text},
        ],
        temperature=0.4,
    )
    return response.choices[0].message.content.strip()

# ---------------------------
# ğŸ§© ìµœì¢… ë¶„ì„ + DB ì €ì¥
# ---------------------------
async def analyze_training_session(title: str, description: str, session_id: int):
    """
    1ï¸âƒ£ Polyglot-Ko ê¸°ë°˜ ë¡œì»¬ LLM + RAGë¡œ ì„¸ì…˜ ë¶„ì„
    2ï¸âƒ£ GPT-4o-minië¡œ ë¬¸ì²´ ë‹¤ë“¬ê¸°
    3ï¸âƒ£ PostgreSQL/SQLiteì— í”¼ë“œë°± ì €ì¥
    """
    query = f"{title}\n{description}"
    raw_output = qa_chain.run(query)
    refined_output = await refine_text(raw_output)

    db_session = SessionLocal()
    try:
        feedback = Feedback(session_id=session_id, ai_feedback=refined_output)
        db_session.add(feedback)
        db_session.commit()
    finally:
        db_session.close()

    return refined_output
