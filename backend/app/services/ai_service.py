import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from sentence_transformers import CrossEncoder
from langchain.schema.runnable import RunnableLambda  # âœ… ì¶”ê°€
from openai import AsyncOpenAI
from app.core.config import settings
from app.core.database import SessionLocal
from app.models.session import Feedback

# ---------------------------
# ğŸ§  í…ŒìŠ¤íŠ¸ìš© GPT2 ëª¨ë¸ ë¡œë“œ
# ---------------------------
print("ğŸ¤– GPT2 ëª¨ë¸ ë¡œë“œ ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

def generate_response(prompt: str) -> str:
    """GPT2 ëª¨ë¸ì„ ì´ìš©í•´ ë¡œì»¬ì—ì„œ ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±"""
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=256)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# ---------------------------
# ğŸ§© ChromaDB + ë¦¬ë­í‚¹ ì„¤ì •
# ---------------------------
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
db = Chroma(persist_directory="vectorstore", embedding_function=embeddings)
retriever = db.as_retriever(search_kwargs={"k": 10})

cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

reranker = CrossEncoderReranker.construct(model=cross_encoder, top_n=3)

compressed_retriever = ContextualCompressionRetriever(
    base_retriever=retriever,
    base_compressor=reranker
)

# ---------------------------
# ğŸ’¬ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
# ---------------------------
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""ë‹¹ì‹ ì€ ì „ë¬¸ íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤.
ì•„ë˜ì˜ ì„¸ì…˜ ê¸°ë¡(context)ì„ ì°¸ê³ í•˜ì—¬ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.

[ì„¸ì…˜ ê¸°ë¡]
{context}

[ì§ˆë¬¸]
{question}

1. í›ˆë ¨ ê°•ë„ í‰ê°€
2. ìì„¸ ë° íš¨ìœ¨ ê°œì„  íŒ
3. ë‹¤ìŒ í›ˆë ¨ ì œì•ˆ
""",
)

# ---------------------------
# ğŸ§  LLM ë˜í¼ (RunnableLambda ì‚¬ìš©)
# ---------------------------
def local_llm(prompt: str) -> str:
    return generate_response(prompt)

LocalLLM = RunnableLambda(local_llm)  # âœ… LangChain 0.3.x í˜¸í™˜

qa_chain = RetrievalQA.from_chain_type(
    llm=LocalLLM,  # âœ… ìˆ˜ì •ë¨
    chain_type="stuff",
    retriever=compressed_retriever,
    chain_type_kwargs={"prompt": prompt},
)

# ---------------------------
# ğŸª„ GPTë¡œ ë¬¸ì²´ ë‹¤ë“¬ê¸°
# ---------------------------
client = AsyncOpenAI(api_key=settings.openai_api_key)

async def refine_text(text: str) -> str:
    """OpenAI GPT ëª¨ë¸ë¡œ ë¬¸ì²´ ìì—°ìŠ¤ëŸ½ê²Œ ë³´ì •"""
    response = await client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ìš´ë™ ì½”ì¹­ í”¼ë“œë°±ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ì–´ ì£¼ì„¸ìš”."},
            {"role": "user", "content": text},
        ],
        temperature=0.4,
    )
    return response.choices[0].message.content.strip()

# ---------------------------
# ğŸ§© ìµœì¢… ë¶„ì„ + DB ì €ì¥
# ---------------------------
async def analyze_training_session(title: str, description: str, session_id: int):
    """ì„¸ì…˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  AI í”¼ë“œë°±ì„ ìƒì„± í›„ DB ì €ì¥"""
    query = f"{title}\n{description}"
    raw_output = qa_chain.run(query)
    refined_output = await refine_text(raw_output)

    db_session = SessionLocal()
    try:
        feedback = Feedback(session_id=session_id, ai_feedback=refined_output)
        db_session.add(feedback)
        db_session.commit()
    finally:
        db_session.close()

    return refined_output
